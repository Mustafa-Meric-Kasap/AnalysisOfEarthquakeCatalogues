{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "o0DPxQ-O82zG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhUPPYA9_Id4",
        "outputId": "00c683fb-fae6-46d2-f099-93a27afb636b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-02e3e9a2b2ae>:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  X_tensor_filtered = torch.load(x_tensor_filtered_path)\n",
            "<ipython-input-2-02e3e9a2b2ae>:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  y_tensor_filtered = torch.load(y_tensor_filtered_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_tensor_filtered: torch.Size([16234, 29, 6])\n",
            "Shape of y_tensor_filtered: torch.Size([16234])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Assume you uploaded files and they are named as follows\n",
        "# x_tensor_path = '/X_tensor.pt'\n",
        "x_tensor_filtered_path = '/content/X_tensor_aftershocks_filtered.pt'\n",
        "# y_tensor_path = '/y_tensor.pt'\n",
        "y_tensor_filtered_path = '/content/y_tensor_aftershocks_filtered.pt'\n",
        "\n",
        "\n",
        "# x_tensor_filtered_path = '/content/X_tensor_pacific_belt_label1.pt'\n",
        "\n",
        "# y_tensor_filtered_path = '/content/y_tensor_pacific_belt_label1.pt'\n",
        "\n",
        "# Loading the tensors\n",
        "# X_tensor = torch.load(x_tensor_path)\n",
        "X_tensor_filtered = torch.load(x_tensor_filtered_path)\n",
        "# y_tensor = torch.load(y_tensor_path)\n",
        "y_tensor_filtered = torch.load(y_tensor_filtered_path)\n",
        "\n",
        "\n",
        "# Display the shape of the tensors\n",
        "# print(\"Shape of X_tensor:\", X_tensor.shape)\n",
        "print(\"Shape of X_tensor_filtered:\", X_tensor_filtered.shape)\n",
        "# print(\"Shape of y_tensor:\", y_tensor.shape)\n",
        "print(\"Shape of y_tensor_filtered:\", y_tensor_filtered.shape)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "mean = X_tensor_filtered.mean(dim=(0, 1))  # Mean for each feature across all samples and time steps\n",
        "std = X_tensor_filtered.std(dim=(0, 1))    # Std dev for each feature across all samples and time steps\n",
        "\n",
        "# Normalize\n",
        "X_tensor_normalized = (X_tensor_filtered - mean[None, None, :]) / std[None, None, :]\n",
        "\n",
        "#X_tensor_normalized = X_tensor_normalized.to(device)\n",
        "#y_tensor_filtered = y_tensor_filtered.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "\n",
        "# Assuming X_tensor_filtered and Y_tensor_filtered are already loaded\n",
        "X = X_tensor_normalized\n",
        "Y = y_tensor_filtered\n",
        "\n",
        "# Create dataset from tensors\n",
        "dataset = TensorDataset(X, Y)\n",
        "\n",
        "# Splitting the dataset into training and validation sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 64  # You can adjust the batch size\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "yjeuZFQ0Bs_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])  # Take the last time step\n",
        "        return out\n",
        "\n",
        "# Initialize the LSTM model\n",
        "input_dim = 6  # number of features\n",
        "hidden_dim = 64\n",
        "num_layers = 2\n",
        "output_dim = 1  # Output dimension (regression)\n",
        "\n",
        "model = LSTMModel(input_dim, hidden_dim, num_layers, output_dim)\n"
      ],
      "metadata": {
        "id": "lfna31yaBy6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs.float())\n",
        "            loss = criterion(outputs.squeeze(), labels.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss = 0\n",
        "            for inputs, labels in val_loader:\n",
        "                outputs = model(inputs.float())\n",
        "                val_loss += criterion(outputs.squeeze(), labels.float()).item()\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}, Val Loss: {val_loss/len(val_loader):.4f}')\n",
        "model = model.to(device)\n",
        "# Train the model\n",
        "num_epochs = 10  # Number of epochs\n",
        "train_model(model, train_loader, val_loader, num_epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYB1sl8-B1Gx",
        "outputId": "c09333dc-205a-4aa2-bccd-aa07054309ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.0001, Val Loss: 0.0019\n",
            "Epoch 2, Loss: 0.0170, Val Loss: 0.0018\n",
            "Epoch 3, Loss: 0.0000, Val Loss: 0.0018\n",
            "Epoch 4, Loss: 0.0000, Val Loss: 0.0018\n",
            "Epoch 5, Loss: 0.0000, Val Loss: 0.0018\n",
            "Epoch 6, Loss: 0.0000, Val Loss: 0.0018\n",
            "Epoch 7, Loss: 0.0000, Val Loss: 0.0019\n",
            "Epoch 8, Loss: 0.0000, Val Loss: 0.0018\n",
            "Epoch 9, Loss: 0.0000, Val Loss: 0.0019\n",
            "Epoch 10, Loss: 0.0168, Val Loss: 0.0018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BinaryLSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
        "        super(BinaryLSTMModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)  # Output dimension is 1 for binary classification\n",
        "        self.sigmoid = nn.Sigmoid()  # Sigmoid activation to output probabilities\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
        "\n",
        "        # Forward propagate LSTM\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "        # Decode the hidden state of the last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "# Initialize the LSTM model for binary classification\n",
        "input_dim = 6  # Number of input features\n",
        "hidden_dim = 64  # Hidden layers dimension\n",
        "num_layers = 2  # Number of LSTM layers\n",
        "\n",
        "model = BinaryLSTMModel(input_dim, hidden_dim, num_layers)"
      ],
      "metadata": {
        "id": "m9lJMNk6JNo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for binary classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs.float())\n",
        "            loss = criterion(outputs.squeeze(), labels.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss = 0\n",
        "            for inputs, labels in val_loader:\n",
        "                outputs = model(inputs.float())\n",
        "                val_loss += criterion(outputs.squeeze(), labels.float()).item()\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}, Val Loss: {val_loss/len(val_loader):.4f}')\n",
        "\n",
        "# Training the model\n",
        "model = model.to(device)\n",
        "num_epochs = 10  # You can adjust the number of epochs\n",
        "train_model(model, train_loader, val_loader, num_epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZ1V91bBJQG5",
        "outputId": "773a38fd-490d-4d3d-b1f7-dbc14baf1a98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.0020, Val Loss: 0.0134\n",
            "Epoch 2, Loss: 0.0010, Val Loss: 0.0137\n",
            "Epoch 3, Loss: 0.0021, Val Loss: 0.0134\n",
            "Epoch 4, Loss: 0.0016, Val Loss: 0.0134\n",
            "Epoch 5, Loss: 0.1092, Val Loss: 0.0134\n",
            "Epoch 6, Loss: 0.0014, Val Loss: 0.0135\n",
            "Epoch 7, Loss: 0.0014, Val Loss: 0.0135\n",
            "Epoch 8, Loss: 0.0013, Val Loss: 0.0135\n",
            "Epoch 9, Loss: 0.0017, Val Loss: 0.0134\n",
            "Epoch 10, Loss: 0.0014, Val Loss: 0.0135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "# Define your LSTM model\n",
        "class BinaryLSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
        "        super(BinaryLSTMModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)  # Output dimension is 1 for binary classification\n",
        "        self.sigmoid = nn.Sigmoid()  # Sigmoid activation to output probabilities\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
        "\n",
        "        # Forward propagate LSTM\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "        # Decode the hidden state of the last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "# Initialize the model\n",
        "input_dim = 6  # Number of input features per earthquake\n",
        "hidden_dim = 64  # Hidden layers dimension\n",
        "num_layers = 2  # Number of LSTM layers\n",
        "\n",
        "model = BinaryLSTMModel(input_dim, hidden_dim, num_layers)\n",
        "model = model.to('cuda')  # Move model to GPU\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for binary classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n",
        "\n",
        "# Prepare data\n",
        "X = X_tensor_filtered  # Shape: (a, b, c) -> a sequences, b earthquakes, c features per earthquake\n",
        "y = y_tensor_filtered  # Shape: (a,) -> Labels (0 or 1) for each sequence\n",
        "\n",
        "# Perform oversampling on the sequences\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_reshaped = X.reshape(X.shape[0], -1)  # Flatten the sequences temporarily to use with RandomOverSampler\n",
        "X_resampled, y_resampled = ros.fit_resample(X_reshaped, y)  # Oversample sequences, not individual earthquakes\n",
        "X_resampled = X_resampled.reshape(-1, X.shape[1], X.shape[2])  # Reshape back to (a, b, c)\n",
        "\n",
        "# Check dimensions to ensure correct reshaping\n",
        "print(f\"Original X shape: {X.shape}\")\n",
        "print(f\"Resampled X shape: {X_resampled.shape}\")\n",
        "print(f\"Resampled y shape: {y_resampled.shape}\")\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
        "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Train the model\n",
        "def train_model(model, train_loader, val_loader, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.squeeze(), labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "                outputs = model(inputs)\n",
        "                val_loss += criterion(outputs.squeeze(), labels).item()\n",
        "                all_preds.append(outputs.cpu().numpy())\n",
        "                all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        all_preds = np.concatenate(all_preds)\n",
        "        all_labels = np.concatenate(all_labels)\n",
        "\n",
        "        # Convert probabilities to binary predictions (0 or 1) using a threshold of 0.5\n",
        "        all_preds_binary = (all_preds > 0.5).astype(int)\n",
        "\n",
        "        # Calculate and print the confusion matrix\n",
        "        cm = confusion_matrix(all_labels, all_preds_binary)\n",
        "        print(f'Confusion Matrix for Epoch {epoch+1}:')\n",
        "        print(cm)\n",
        "\n",
        "        # Compute ROC AUC score\n",
        "        auc = roc_auc_score(all_labels, all_preds)\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}, ROC AUC: {auc:.4f}')\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10  # You can adjust the number of epochs\n",
        "train_model(model, train_loader, val_loader, num_epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJ_Mxi6P96No",
        "outputId": "b0871ee9-86f8-40ff-94ce-c5eb3322b1a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original X shape: torch.Size([16234, 29, 6])\n",
            "Resampled X shape: (32412, 29, 6)\n",
            "Resampled y shape: (32412,)\n",
            "Confusion Matrix for Epoch 1:\n",
            "[[3224   53]\n",
            " [   0 3206]]\n",
            "Epoch 1, Loss: 0.2266, Val Loss: 0.0362, ROC AUC: 0.9980\n",
            "Confusion Matrix for Epoch 2:\n",
            "[[3277    0]\n",
            " [2152 1054]]\n",
            "Epoch 2, Loss: 0.0191, Val Loss: 1.0120, ROC AUC: 0.9987\n",
            "Confusion Matrix for Epoch 3:\n",
            "[[3269    8]\n",
            " [   0 3206]]\n",
            "Epoch 3, Loss: 0.0193, Val Loss: 0.0180, ROC AUC: 0.9997\n",
            "Confusion Matrix for Epoch 4:\n",
            "[[3266   11]\n",
            " [   0 3206]]\n",
            "Epoch 4, Loss: 0.0084, Val Loss: 0.0051, ROC AUC: 1.0000\n",
            "Confusion Matrix for Epoch 5:\n",
            "[[3272    5]\n",
            " [   0 3206]]\n",
            "Epoch 5, Loss: 0.0100, Val Loss: 0.0042, ROC AUC: 0.9998\n",
            "Confusion Matrix for Epoch 6:\n",
            "[[3268    9]\n",
            " [   0 3206]]\n",
            "Epoch 6, Loss: 0.0052, Val Loss: 0.0062, ROC AUC: 0.9999\n",
            "Confusion Matrix for Epoch 7:\n",
            "[[3272    5]\n",
            " [   0 3206]]\n",
            "Epoch 7, Loss: 0.0044, Val Loss: 0.0028, ROC AUC: 1.0000\n",
            "Confusion Matrix for Epoch 8:\n",
            "[[3272    5]\n",
            " [   0 3206]]\n",
            "Epoch 8, Loss: 0.0020, Val Loss: 0.0030, ROC AUC: 1.0000\n",
            "Confusion Matrix for Epoch 9:\n",
            "[[3272    5]\n",
            " [   0 3206]]\n",
            "Epoch 9, Loss: 0.0060, Val Loss: 0.0039, ROC AUC: 1.0000\n",
            "Confusion Matrix for Epoch 10:\n",
            "[[3273    4]\n",
            " [   0 3206]]\n",
            "Epoch 10, Loss: 0.0023, Val Loss: 0.0021, ROC AUC: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a GRU model for binary classification\n",
        "class BinaryGRUModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
        "        super(BinaryGRUModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)  # Output dimension is 1 for binary classification\n",
        "        self.sigmoid = nn.Sigmoid()  # Sigmoid activation to output probabilities\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
        "\n",
        "        # Forward propagate GRU\n",
        "        out, _ = self.gru(x, h0)\n",
        "\n",
        "        # Decode the hidden state of the last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "# Initialize the GRU model\n",
        "gru_model = BinaryGRUModel(input_dim, hidden_dim, num_layers)\n",
        "gru_model = gru_model.to('cuda')  # Move model to GPU\n",
        "\n",
        "# Define loss and optimizer for GRU model\n",
        "gru_optimizer = optim.Adam(gru_model.parameters(), lr=0.001)  # Adam optimizer for GRU model\n",
        "\n",
        "# Reuse the `train_model` function\n",
        "def train_gru_model(model, train_loader, val_loader, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "            gru_optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.squeeze(), labels)\n",
        "            loss.backward()\n",
        "            gru_optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "                outputs = model(inputs)\n",
        "                val_loss += criterion(outputs.squeeze(), labels).item()\n",
        "                all_preds.append(outputs.cpu().numpy())\n",
        "                all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        all_preds = np.concatenate(all_preds)\n",
        "        all_labels = np.concatenate(all_labels)\n",
        "\n",
        "        # Convert probabilities to binary predictions (0 or 1) using a threshold of 0.5\n",
        "        all_preds_binary = (all_preds > 0.5).astype(int)\n",
        "\n",
        "        # Calculate and print the confusion matrix\n",
        "        cm = confusion_matrix(all_labels, all_preds_binary)\n",
        "        print(f'Confusion Matrix for Epoch {epoch+1}:')\n",
        "        print(cm)\n",
        "\n",
        "        # Compute ROC AUC score\n",
        "        auc = roc_auc_score(all_labels, all_preds)\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}, ROC AUC: {auc:.4f}')\n",
        "\n",
        "# Train the GRU model\n",
        "num_epochs = 10  # Adjust as needed\n",
        "train_gru_model(gru_model, train_loader, val_loader, num_epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6RLyL28o4xw",
        "outputId": "168b5092-bb93-42c2-9d38-598a417c1799"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix for Epoch 1:\n",
            "[[3260   17]\n",
            " [   0 3206]]\n",
            "Epoch 1, Loss: 0.1746, Val Loss: 0.0123, ROC AUC: 0.9995\n",
            "Confusion Matrix for Epoch 2:\n",
            "[[3271    6]\n",
            " [   0 3206]]\n",
            "Epoch 2, Loss: 0.0129, Val Loss: 0.0055, ROC AUC: 0.9999\n",
            "Confusion Matrix for Epoch 3:\n",
            "[[3256   21]\n",
            " [   0 3206]]\n",
            "Epoch 3, Loss: 0.0068, Val Loss: 0.0125, ROC AUC: 0.9999\n",
            "Confusion Matrix for Epoch 4:\n",
            "[[3268    9]\n",
            " [   0 3206]]\n",
            "Epoch 4, Loss: 0.0088, Val Loss: 0.0041, ROC AUC: 1.0000\n",
            "Confusion Matrix for Epoch 5:\n",
            "[[3275    2]\n",
            " [   0 3206]]\n",
            "Epoch 5, Loss: 0.0038, Val Loss: 0.0017, ROC AUC: 1.0000\n",
            "Confusion Matrix for Epoch 6:\n",
            "[[3272    5]\n",
            " [   0 3206]]\n",
            "Epoch 6, Loss: 0.0063, Val Loss: 0.0025, ROC AUC: 1.0000\n",
            "Confusion Matrix for Epoch 7:\n",
            "[[3261   16]\n",
            " [   0 3206]]\n",
            "Epoch 7, Loss: 0.0069, Val Loss: 0.0080, ROC AUC: 0.9998\n",
            "Confusion Matrix for Epoch 8:\n",
            "[[3272    5]\n",
            " [   0 3206]]\n",
            "Epoch 8, Loss: 0.0023, Val Loss: 0.0029, ROC AUC: 1.0000\n",
            "Confusion Matrix for Epoch 9:\n",
            "[[3273    4]\n",
            " [   0 3206]]\n",
            "Epoch 9, Loss: 0.0021, Val Loss: 0.0030, ROC AUC: 0.9997\n",
            "Confusion Matrix for Epoch 10:\n",
            "[[3274    3]\n",
            " [   0 3206]]\n",
            "Epoch 10, Loss: 0.0015, Val Loss: 0.0019, ROC AUC: 0.9998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a CNN model for binary classification\n",
        "import torch.nn.functional as F\n",
        "class BinaryCNNModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_channels, kernel_size):\n",
        "        super(BinaryCNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=num_channels, kernel_size=kernel_size)\n",
        "        self.conv2 = nn.Conv1d(in_channels=num_channels, out_channels=num_channels*2, kernel_size=kernel_size)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "        self.fc1 = nn.Linear(320, 64)  # Adjust based on actual flattened length\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # Change shape to [batch_size, num_features, num_earthquakes]\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)  # Flatten the output\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.sigmoid(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "# Initialize the CNN model\n",
        "input_dim = 6  # Number of features per earthquake\n",
        "num_channels = 32  # Number of output channels for the first convolution layer\n",
        "kernel_size = 3  # Kernel size for the convolution layers\n",
        "\n",
        "cnn_model = BinaryCNNModel(input_dim, num_channels, kernel_size)\n",
        "cnn_model = cnn_model.to('cuda')  # Move model to GPU\n",
        "\n",
        "# Define loss and optimizer for CNN model\n",
        "cnn_optimizer = optim.Adam(cnn_model.parameters(), lr=0.0001)  # Adam optimizer for CNN model\n",
        "\n",
        "# Reuse the `train_model` function\n",
        "def train_cnn_model(model, train_loader, val_loader, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "            cnn_optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.squeeze(), labels)\n",
        "            loss.backward()\n",
        "            cnn_optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "                outputs = model(inputs)\n",
        "                val_loss += criterion(outputs.squeeze(), labels).item()\n",
        "                all_preds.append(outputs.cpu().numpy())\n",
        "                all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        all_preds = np.concatenate(all_preds)\n",
        "        all_labels = np.concatenate(all_labels)\n",
        "\n",
        "        # Convert probabilities to binary predictions (0 or 1) using a threshold of 0.5\n",
        "        all_preds_binary = (all_preds > 0.5).astype(int)\n",
        "\n",
        "        # Calculate and print the confusion matrix\n",
        "        cm = confusion_matrix(all_labels, all_preds_binary)\n",
        "\n",
        "\n",
        "        # Compute ROC AUC score\n",
        "        auc = roc_auc_score(all_labels, all_preds)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "                print(f'Epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}, Val Loss: {val_loss:.4f}, ROC AUC: {auc:.4f}')\n",
        "                print(f'Confusion Matrix for Epoch {epoch+1}:')\n",
        "                print(cm)\n",
        "# Train the CNN model\n",
        "num_epochs = 100  # Adjust as needed\n",
        "train_cnn_model(cnn_model, train_loader, val_loader, num_epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpehTQgXqG0s",
        "outputId": "f8b58e93-0aeb-4ca9-d70a-efad16dc3b32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Loss: 1.9829, Val Loss: 1.6605, ROC AUC: 0.9729\n",
            "Confusion Matrix for Epoch 10:\n",
            "[[3276    1]\n",
            " [ 108 3098]]\n",
            "Epoch 20, Loss: 3.4141, Val Loss: 3.2046, ROC AUC: 0.9389\n",
            "Confusion Matrix for Epoch 20:\n",
            "[[3272    5]\n",
            " [ 335 2871]]\n",
            "Epoch 30, Loss: 3.3198, Val Loss: 3.2679, ROC AUC: 0.9525\n",
            "Confusion Matrix for Epoch 30:\n",
            "[[3277    0]\n",
            " [ 240 2966]]\n",
            "Epoch 40, Loss: 3.5130, Val Loss: 3.4198, ROC AUC: 0.9530\n",
            "Confusion Matrix for Epoch 40:\n",
            "[[3277    0]\n",
            " [ 240 2966]]\n",
            "Epoch 50, Loss: 3.8066, Val Loss: 3.7129, ROC AUC: 0.9435\n",
            "Confusion Matrix for Epoch 50:\n",
            "[[3277    0]\n",
            " [ 240 2966]]\n",
            "Epoch 60, Loss: 3.7831, Val Loss: 3.7139, ROC AUC: 0.9440\n",
            "Confusion Matrix for Epoch 60:\n",
            "[[3276    1]\n",
            " [ 240 2966]]\n",
            "Epoch 70, Loss: 3.7831, Val Loss: 3.7141, ROC AUC: 0.9444\n",
            "Confusion Matrix for Epoch 70:\n",
            "[[3276    1]\n",
            " [ 240 2966]]\n",
            "Epoch 80, Loss: 1.9093, Val Loss: 1.6709, ROC AUC: 0.9744\n",
            "Confusion Matrix for Epoch 80:\n",
            "[[3275    2]\n",
            " [ 108 3098]]\n",
            "Epoch 90, Loss: 1.8858, Val Loss: 1.6851, ROC AUC: 0.9742\n",
            "Confusion Matrix for Epoch 90:\n",
            "[[3275    2]\n",
            " [ 108 3098]]\n",
            "Epoch 100, Loss: 1.8858, Val Loss: 1.6851, ROC AUC: 0.9743\n",
            "Confusion Matrix for Epoch 100:\n",
            "[[3275    2]\n",
            " [ 108 3098]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import numpy as np\n",
        "\n",
        "# Move your data to CPU\n",
        "X = X_tensor_filtered.cpu().numpy().reshape(X_tensor_filtered.size(0), -1)  # Flatten sequences\n",
        "y = y_tensor_filtered.cpu().numpy()\n",
        "\n",
        "# Perform oversampling on the sequences\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
        "\n",
        "# Split the resampled data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Random Forest model\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,            # Number of trees in the forest\n",
        "    max_depth=5,                # Maximum depth of each tree\n",
        "    min_samples_split=5,         # Minimum number of samples required to split an internal node\n",
        "    min_samples_leaf=3,          # Minimum number of samples required to be at a leaf node\n",
        "    max_features='sqrt',         # Number of features to consider when looking for the best split\n",
        "    random_state=42              # For reproducibility\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_proba = rf_model.predict_proba(X_val)[:, 1]  # Get probabilities for the positive class\n",
        "y_pred = rf_model.predict(X_val)\n",
        "\n",
        "# Compute ROC AUC and Accuracy\n",
        "roc_auc = roc_auc_score(y_val, y_pred_proba)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "\n",
        "print(f'Random Forest ROC AUC: {roc_auc:.4f}')\n",
        "print(f'Random Forest Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Compute Confusion Matrix\n",
        "cm = confusion_matrix(y_val, y_pred)\n",
        "print(cm)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpWtcfiWudKJ",
        "outputId": "fe651ebd-dcff-456a-94a1-6df6322f043b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest ROC AUC: 1.0000\n",
            "Random Forest Accuracy: 0.9980\n",
            "[[3264   13]\n",
            " [   0 3206]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define a CNN model with dilation for binary classification\n",
        "class BinaryCNNModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_channels, kernel_size, dilation_rate=1):\n",
        "        super(BinaryCNNModel, self).__init__()\n",
        "        # Adding dilation to convolutional layers\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=num_channels,\n",
        "                               kernel_size=kernel_size, dilation=dilation_rate)\n",
        "        self.conv2 = nn.Conv1d(in_channels=num_channels, out_channels=num_channels * 2,\n",
        "                               kernel_size=kernel_size, dilation=dilation_rate)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "        self.fc1 = nn.Linear(64 * 4, 64)  # Adjusted based on the flattened size\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # Change shape to [batch_size, num_features, num_earthquakes]\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # Flatten the output\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.sigmoid(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Initialize the CNN model with dilation\n",
        "input_dim = 6  # Number of features per earthquake\n",
        "num_channels = 32  # Number of output channels for the first convolution layer\n",
        "kernel_size = 3  # Kernel size for the convolution layers\n",
        "dilation_rate = 2  # Dilation rate for the convolution layers\n",
        "\n",
        "cnn_model = BinaryCNNModel(input_dim, num_channels, kernel_size, dilation_rate)\n",
        "# cnn_model = cnn_model.to('cuda')  # Move model to GPU\n",
        "\n",
        "# Define loss and optimizer for CNN model\n",
        "criterion = nn.BCELoss()\n",
        "cnn_optimizer = optim.Adam(cnn_model.parameters(), lr=0.0001)  # Adam optimizer for CNN model\n",
        "\n",
        "# Reuse the `train_model` function\n",
        "def train_cnn_model(model, train_loader, val_loader, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            # inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "            cnn_optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.squeeze(), labels)\n",
        "            loss.backward()\n",
        "            cnn_optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                # inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "                outputs = model(inputs)\n",
        "                val_loss += criterion(outputs.squeeze(), labels).item()\n",
        "                all_preds.append(outputs.cpu().numpy())\n",
        "                all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        all_preds = np.concatenate(all_preds)\n",
        "        all_labels = np.concatenate(all_labels)\n",
        "\n",
        "        # Convert probabilities to binary predictions (0 or 1) using a threshold of 0.5\n",
        "        all_preds_binary = (all_preds > 0.5).astype(int)\n",
        "\n",
        "        # Calculate and print the confusion matrix\n",
        "        cm = confusion_matrix(all_labels, all_preds_binary)\n",
        "\n",
        "        # Compute ROC AUC score\n",
        "        auc = roc_auc_score(all_labels, all_preds)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}, Val Loss: {val_loss:.4f}, ROC AUC: {auc:.4f}')\n",
        "            print(f'Confusion Matrix for Epoch {epoch+1}:')\n",
        "            print(cm)\n",
        "\n",
        "# Train the CNN model\n",
        "num_epochs = 100  # Adjust as needed\n",
        "train_cnn_model(cnn_model, train_loader, val_loader, num_epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ci6MsANJPUyW",
        "outputId": "93fa6fd7-ed6b-405c-e42c-0e20197eeec1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Loss: 0.0141, Val Loss: 0.0126, ROC AUC: 0.4988\n",
            "Confusion Matrix for Epoch 10:\n",
            "[[3242    0]\n",
            " [   5    0]]\n",
            "Epoch 20, Loss: 0.0130, Val Loss: 0.0120, ROC AUC: 0.5195\n",
            "Confusion Matrix for Epoch 20:\n",
            "[[3242    0]\n",
            " [   5    0]]\n",
            "Epoch 30, Loss: 0.0112, Val Loss: 0.0120, ROC AUC: 0.5308\n",
            "Confusion Matrix for Epoch 30:\n",
            "[[3242    0]\n",
            " [   5    0]]\n",
            "Epoch 40, Loss: 0.0080, Val Loss: 0.0124, ROC AUC: 0.5381\n",
            "Confusion Matrix for Epoch 40:\n",
            "[[3242    0]\n",
            " [   5    0]]\n",
            "Epoch 50, Loss: 0.0048, Val Loss: 0.0135, ROC AUC: 0.5611\n",
            "Confusion Matrix for Epoch 50:\n",
            "[[3242    0]\n",
            " [   5    0]]\n",
            "Epoch 60, Loss: 0.0025, Val Loss: 0.0150, ROC AUC: 0.5698\n",
            "Confusion Matrix for Epoch 60:\n",
            "[[3242    0]\n",
            " [   5    0]]\n",
            "Epoch 70, Loss: 0.0009, Val Loss: 0.0176, ROC AUC: 0.5522\n",
            "Confusion Matrix for Epoch 70:\n",
            "[[3241    1]\n",
            " [   5    0]]\n",
            "Epoch 80, Loss: 0.0002, Val Loss: 0.0223, ROC AUC: 0.5524\n",
            "Confusion Matrix for Epoch 80:\n",
            "[[3241    1]\n",
            " [   5    0]]\n",
            "Epoch 90, Loss: 0.0001, Val Loss: 0.0256, ROC AUC: 0.5571\n",
            "Confusion Matrix for Epoch 90:\n",
            "[[3241    1]\n",
            " [   5    0]]\n",
            "Epoch 100, Loss: 0.0000, Val Loss: 0.0308, ROC AUC: 0.5590\n",
            "Confusion Matrix for Epoch 100:\n",
            "[[3241    1]\n",
            " [   5    0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Define a CNN model with dilation for binary classification\n",
        "class BinaryCNNModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_channels, kernel_size, dilation_rate=1):\n",
        "        super(BinaryCNNModel, self).__init__()\n",
        "        # Adding dilation to convolutional layers\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=num_channels,\n",
        "                               kernel_size=kernel_size, dilation=dilation_rate)\n",
        "        self.conv2 = nn.Conv1d(in_channels=num_channels, out_channels=num_channels * 2,\n",
        "                               kernel_size=kernel_size, dilation=dilation_rate)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "        self.fc1 = nn.Linear(64 * 4, 64)  # Adjusted based on the flattened size\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # Change shape to [batch_size, num_features, num_earthquakes]\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # Flatten the output\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.sigmoid(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "# Initialize the CNN model with dilation\n",
        "input_dim = 6  # Number of features per earthquake\n",
        "num_channels = 32  # Number of output channels for the first convolution layer\n",
        "kernel_size = 3  # Kernel size for the convolution layers\n",
        "dilation_rate = 2  # Dilation rate for the convolution layers\n",
        "\n",
        "cnn_model = BinaryCNNModel(input_dim, num_channels, kernel_size, dilation_rate)\n",
        "\n",
        "# Define loss and optimizer for CNN model\n",
        "criterion = nn.BCELoss()\n",
        "cnn_optimizer = optim.Adam(cnn_model.parameters(), lr=0.0001)  # Adam optimizer for CNN model\n",
        "\n",
        "# Prepare data\n",
        "X = X_tensor_filtered  # Shape: (a, b, c) -> a sequences, b earthquakes, c features per earthquake\n",
        "y = y_tensor_filtered  # Shape: (a,) -> Labels (0 or 1) for each sequence\n",
        "\n",
        "# Perform oversampling on the sequences\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_reshaped = X.reshape(X.shape[0], -1)  # Flatten the sequences temporarily to use with RandomOverSampler\n",
        "X_resampled, y_resampled = ros.fit_resample(X_reshaped, y)  # Oversample sequences, not individual earthquakes\n",
        "X_resampled = X_resampled.reshape(-1, X.shape[1], X.shape[2])  # Reshape back to (a, b, c)\n",
        "\n",
        "# Check dimensions to ensure correct reshaping\n",
        "print(f\"Original X shape: {X.shape}\")\n",
        "print(f\"Resampled X shape: {X_resampled.shape}\")\n",
        "print(f\"Resampled y shape: {y_resampled.shape}\")\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
        "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Train the CNN model\n",
        "def train_cnn_model(model, train_loader, val_loader, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            # inputs, labels = inputs.to('cuda'), labels.to('cuda')  # Uncomment if using GPU\n",
        "            cnn_optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.squeeze(), labels)\n",
        "            loss.backward()\n",
        "            cnn_optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                # inputs, labels = inputs.to('cuda'), labels.to('cuda')  # Uncomment if using GPU\n",
        "                outputs = model(inputs)\n",
        "                val_loss += criterion(outputs.squeeze(), labels).item()\n",
        "                all_preds.append(outputs.cpu().numpy())\n",
        "                all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        all_preds = np.concatenate(all_preds)\n",
        "        all_labels = np.concatenate(all_labels)\n",
        "\n",
        "        # Convert probabilities to binary predictions (0 or 1) using a threshold of 0.5\n",
        "        all_preds_binary = (all_preds > 0.5).astype(int)\n",
        "\n",
        "        # Calculate and print the confusion matrix\n",
        "\n",
        "        # print(f'Confusion Matrix for Epoch {epoch+1}:')\n",
        "\n",
        "\n",
        "        # Compute ROC AUC score\n",
        "        auc = roc_auc_score(all_labels, all_preds)\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}, Val Loss: {val_loss:.4f}, ROC AUC: {auc:.4f}')\n",
        "            cm = confusion_matrix(all_labels, all_preds_binary)\n",
        "            print(cm)\n",
        "# Train the CNN model\n",
        "num_epochs = 100  # Adjust as needed\n",
        "train_cnn_model(cnn_model, train_loader, val_loader, num_epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s--Q8W3HSHB4",
        "outputId": "4379e50e-e03c-47d9-c9d3-38a60a91e88a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original X shape: torch.Size([16234, 29, 6])\n",
            "Resampled X shape: (32412, 29, 6)\n",
            "Resampled y shape: (32412,)\n",
            "Epoch 10, Loss: 0.0688, Val Loss: 0.0087, ROC AUC: 0.9999\n",
            "[[3273    4]\n",
            " [   0 3206]]\n",
            "Epoch 20, Loss: 0.0007, Val Loss: 0.0024, ROC AUC: 0.9999\n",
            "[[3273    4]\n",
            " [   0 3206]]\n",
            "Epoch 30, Loss: 0.0032, Val Loss: 0.0035, ROC AUC: 1.0000\n",
            "[[3272    5]\n",
            " [   0 3206]]\n",
            "Epoch 40, Loss: 0.0001, Val Loss: 0.0043, ROC AUC: 0.9999\n",
            "[[3272    5]\n",
            " [   0 3206]]\n",
            "Epoch 50, Loss: 0.0002, Val Loss: 0.0029, ROC AUC: 0.9999\n",
            "[[3274    3]\n",
            " [   0 3206]]\n",
            "Epoch 60, Loss: 0.0000, Val Loss: 0.0033, ROC AUC: 0.9999\n",
            "[[3275    2]\n",
            " [   0 3206]]\n",
            "Epoch 70, Loss: 0.0000, Val Loss: 0.0164, ROC AUC: 0.9998\n",
            "[[3275    2]\n",
            " [   0 3206]]\n",
            "Epoch 80, Loss: 0.0039, Val Loss: 0.0035, ROC AUC: 0.9999\n",
            "[[3273    4]\n",
            " [   0 3206]]\n",
            "Epoch 90, Loss: 0.0039, Val Loss: 0.0031, ROC AUC: 1.0000\n",
            "[[3273    4]\n",
            " [   0 3206]]\n",
            "Epoch 100, Loss: 0.0039, Val Loss: 0.0024, ROC AUC: 1.0000\n",
            "[[3274    3]\n",
            " [   0 3206]]\n"
          ]
        }
      ]
    }
  ]
}