{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTSIdRYYhFIW",
        "outputId": "1f596db7-567f-44f1-9c1b-d7621e38479e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_tensor_filtered shape: torch.Size([16234, 29, 6])\n",
            "y_tensor_filtered shape: torch.Size([16234])\n",
            "x_tensor_filtered_1 shape: torch.Size([1043, 29, 6])\n",
            "y_tensor_filtered_1 shape: torch.Size([1043])\n",
            "x_combined shape: torch.Size([2086, 29, 6])\n",
            "y_combined shape: torch.Size([2086])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-c3b08e72cec0>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  x_tensor_filtered = torch.load('/content/X_tensor_aftershocks_filtered.pt')\n",
            "<ipython-input-9-c3b08e72cec0>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  y_tensor_filtered = torch.load('/content/y_tensor_aftershocks_filtered.pt')\n",
            "<ipython-input-9-c3b08e72cec0>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  x_tensor_filtered_1 = torch.load('/content/X_tensor_pacific_belt_label1.pt')\n",
            "<ipython-input-9-c3b08e72cec0>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  y_tensor_filtered_1 = torch.load('/content/y_tensor_pacific_belt_label1.pt')\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Load the tensors\n",
        "x_tensor_filtered = torch.load('/content/X_tensor_aftershocks_filtered.pt')\n",
        "y_tensor_filtered = torch.load('/content/y_tensor_aftershocks_filtered.pt')\n",
        "\n",
        "x_tensor_filtered_1 = torch.load('/content/X_tensor_pacific_belt_label1.pt')\n",
        "y_tensor_filtered_1 = torch.load('/content/y_tensor_pacific_belt_label1.pt')\n",
        "\n",
        "# Print the shapes\n",
        "print(f\"x_tensor_filtered shape: {x_tensor_filtered.shape}\")\n",
        "print(f\"y_tensor_filtered shape: {y_tensor_filtered.shape}\")\n",
        "\n",
        "print(f\"x_tensor_filtered_1 shape: {x_tensor_filtered_1.shape}\")\n",
        "print(f\"y_tensor_filtered_1 shape: {y_tensor_filtered_1.shape}\")\n",
        "\n",
        "# Determine the length of the smaller dataset\n",
        "length_1 = len(x_tensor_filtered_1)\n",
        "\n",
        "# Randomly sample from the larger dataset\n",
        "indices = torch.randperm(len(x_tensor_filtered))[:length_1]\n",
        "x_tensor_filtered_sampled = x_tensor_filtered[indices]\n",
        "y_tensor_filtered_sampled = y_tensor_filtered[indices]\n",
        "\n",
        "# Concatenate the two datasets\n",
        "x_combined = torch.cat((x_tensor_filtered_sampled, x_tensor_filtered_1), dim=0)\n",
        "y_combined = torch.cat((y_tensor_filtered_sampled, y_tensor_filtered_1), dim=0)\n",
        "\n",
        "# Print the shapes of the combined datasets\n",
        "print(f\"x_combined shape: {x_combined.shape}\")\n",
        "print(f\"y_combined shape: {y_combined.shape}\")\n",
        "\n",
        "mean = x_combined.mean(dim=(0, 1))  # Mean for each feature across all samples and time steps\n",
        "std = x_combined.std(dim=(0, 1))    # Std dev for each feature across all samples and time steps\n",
        "\n",
        "# Normalize\n",
        "x_combined_normalized = (x_combined - mean[None, None, :]) / std[None, None, :]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "\n",
        "# Assuming X_tensor_filtered and Y_tensor_filtered are already loaded\n",
        "X = x_combined_normalized\n",
        "Y = y_combined\n",
        "\n",
        "# Create dataset from tensors\n",
        "dataset = TensorDataset(X, Y)\n",
        "\n",
        "# Splitting the dataset into training and validation sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 64  # You can adjust the batch size\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "0n-90BKgjeGO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])  # Take the last time step\n",
        "        return out\n",
        "\n",
        "# Initialize the LSTM model\n",
        "input_dim = 6  # number of features\n",
        "hidden_dim = 64\n",
        "num_layers = 2\n",
        "output_dim = 1  # Output dimension (regression)\n",
        "\n",
        "model = LSTMModel(input_dim, hidden_dim, num_layers, output_dim)\n"
      ],
      "metadata": {
        "id": "oEGZ6JppkQMJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs.float())\n",
        "            loss = criterion(outputs.squeeze(), labels.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss = 0\n",
        "            for inputs, labels in val_loader:\n",
        "                outputs = model(inputs.float())\n",
        "                val_loss += criterion(outputs.squeeze(), labels.float()).item()\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}, Val Loss: {val_loss/len(val_loader):.4f}')\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10  # Number of epochs\n",
        "train_model(model, train_loader, val_loader, num_epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tf2vIWyhkSHj",
        "outputId": "aa8749e6-6d1a-4e05-b0a9-2872fec79e5e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.2674, Val Loss: 0.2432\n",
            "Epoch 2, Loss: 0.1832, Val Loss: 0.1965\n",
            "Epoch 3, Loss: 0.0847, Val Loss: 0.0600\n",
            "Epoch 4, Loss: 0.0128, Val Loss: 0.0186\n",
            "Epoch 5, Loss: 0.0065, Val Loss: 0.0144\n",
            "Epoch 6, Loss: 0.0626, Val Loss: 0.0063\n",
            "Epoch 7, Loss: 0.0002, Val Loss: 0.0037\n",
            "Epoch 8, Loss: 0.0009, Val Loss: 0.0025\n",
            "Epoch 9, Loss: 0.0003, Val Loss: 0.0021\n",
            "Epoch 10, Loss: 0.0000, Val Loss: 0.0011\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BinaryLSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
        "        super(BinaryLSTMModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)  # Output dimension is 1 for binary classification\n",
        "        self.sigmoid = nn.Sigmoid()  # Sigmoid activation to output probabilities\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
        "\n",
        "        # Forward propagate LSTM\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "        # Decode the hidden state of the last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "# Initialize the LSTM model for binary classification\n",
        "input_dim = 6  # Number of input features\n",
        "hidden_dim = 64  # Hidden layers dimension\n",
        "num_layers = 2  # Number of LSTM layers\n",
        "\n",
        "model = BinaryLSTMModel(input_dim, hidden_dim, num_layers)\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for binary classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs.float())\n",
        "            loss = criterion(outputs.squeeze(), labels.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss = 0\n",
        "            for inputs, labels in val_loader:\n",
        "                outputs = model(inputs.float())\n",
        "                val_loss += criterion(outputs.squeeze(), labels.float()).item()\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}, Val Loss: {val_loss/len(val_loader):.4f}')\n",
        "\n",
        "# Training the model\n",
        "\n",
        "num_epochs = 10  # You can adjust the number of epochs\n",
        "train_model(model, train_loader, val_loader, num_epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BvkY8gHlcFj",
        "outputId": "9cfc412a-c22b-4f35-d14f-5a3956116510"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.4408, Val Loss: 0.6014\n",
            "Epoch 2, Loss: 0.2613, Val Loss: 0.3032\n",
            "Epoch 3, Loss: 0.0691, Val Loss: 0.1128\n",
            "Epoch 4, Loss: 0.0248, Val Loss: 0.0484\n",
            "Epoch 5, Loss: 0.0053, Val Loss: 0.0446\n",
            "Epoch 6, Loss: 0.0048, Val Loss: 0.0234\n",
            "Epoch 7, Loss: 0.0028, Val Loss: 0.0149\n",
            "Epoch 8, Loss: 0.0018, Val Loss: 0.0075\n",
            "Epoch 9, Loss: 0.0019, Val Loss: 0.0031\n",
            "Epoch 10, Loss: 0.0011, Val Loss: 0.0053\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Define your LSTM model\n",
        "class BinaryLSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
        "        super(BinaryLSTMModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)  # Output dimension is 1 for binary classification\n",
        "        self.sigmoid = nn.Sigmoid()  # Sigmoid activation to output probabilities\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
        "\n",
        "        # Forward propagate LSTM\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "        # Decode the hidden state of the last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "# Initialize the model\n",
        "input_dim = 6  # Number of input features per earthquake\n",
        "hidden_dim = 64  # Hidden layers dimension\n",
        "num_layers = 2  # Number of LSTM layers\n",
        "\n",
        "model = BinaryLSTMModel(input_dim, hidden_dim, num_layers)\n",
        "\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for binary classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(x_combined_normalized, y_combined, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
        "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Train the model\n",
        "def train_model(model, train_loader, val_loader, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for inputs, labels in train_loader:\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.squeeze(), labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                val_loss += criterion(outputs.squeeze(), labels).item()\n",
        "                all_preds.append(outputs.cpu().numpy())\n",
        "                all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        all_preds = np.concatenate(all_preds)\n",
        "        all_labels = np.concatenate(all_labels)\n",
        "\n",
        "        # Convert probabilities to binary predictions (0 or 1) using a threshold of 0.5\n",
        "        all_preds_binary = (all_preds > 0.5).astype(int)\n",
        "\n",
        "        # Calculate and print the confusion matrix\n",
        "        cm = confusion_matrix(all_labels, all_preds_binary)\n",
        "        print(f'Confusion Matrix for Epoch {epoch+1}:')\n",
        "        print(cm)\n",
        "\n",
        "        # Compute ROC AUC score\n",
        "        auc = roc_auc_score(all_labels, all_preds)\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}, ROC AUC: {auc:.4f}')\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10  # You can adjust the number of epochs\n",
        "train_model(model, train_loader, val_loader, num_epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5o7jo4rmSVs",
        "outputId": "3579617d-acfa-4591-b08c-bce4dcfe0b67"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-53720dd92937>:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
            "<ipython-input-21-53720dd92937>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix for Epoch 1:\n",
            "[[163  50]\n",
            " [ 67 138]]\n",
            "Epoch 1, Loss: 0.6820, Val Loss: 0.6349, ROC AUC: 0.8109\n",
            "Confusion Matrix for Epoch 2:\n",
            "[[180  33]\n",
            " [ 19 186]]\n",
            "Epoch 2, Loss: 0.5001, Val Loss: 0.3481, ROC AUC: 0.9510\n",
            "Confusion Matrix for Epoch 3:\n",
            "[[207   6]\n",
            " [  3 202]]\n",
            "Epoch 3, Loss: 0.1940, Val Loss: 0.0872, ROC AUC: 0.9967\n",
            "Confusion Matrix for Epoch 4:\n",
            "[[210   3]\n",
            " [  3 202]]\n",
            "Epoch 4, Loss: 0.0775, Val Loss: 0.0582, ROC AUC: 0.9991\n",
            "Confusion Matrix for Epoch 5:\n",
            "[[210   3]\n",
            " [  0 205]]\n",
            "Epoch 5, Loss: 0.0552, Val Loss: 0.0322, ROC AUC: 0.9997\n",
            "Confusion Matrix for Epoch 6:\n",
            "[[210   3]\n",
            " [  0 205]]\n",
            "Epoch 6, Loss: 0.0297, Val Loss: 0.0284, ROC AUC: 1.0000\n",
            "Confusion Matrix for Epoch 7:\n",
            "[[213   0]\n",
            " [  0 205]]\n",
            "Epoch 7, Loss: 0.0667, Val Loss: 0.0074, ROC AUC: 1.0000\n",
            "Confusion Matrix for Epoch 8:\n",
            "[[211   2]\n",
            " [  0 205]]\n",
            "Epoch 8, Loss: 0.0677, Val Loss: 0.0316, ROC AUC: 1.0000\n",
            "Confusion Matrix for Epoch 9:\n",
            "[[213   0]\n",
            " [  0 205]]\n",
            "Epoch 9, Loss: 0.0148, Val Loss: 0.0037, ROC AUC: 1.0000\n",
            "Confusion Matrix for Epoch 10:\n",
            "[[213   0]\n",
            " [  0 205]]\n",
            "Epoch 10, Loss: 0.0076, Val Loss: 0.0030, ROC AUC: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a GRU model for binary classification\n",
        "class BinaryGRUModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
        "        super(BinaryGRUModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)  # Output dimension is 1 for binary classification\n",
        "        self.sigmoid = nn.Sigmoid()  # Sigmoid activation to output probabilities\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
        "\n",
        "        # Forward propagate GRU\n",
        "        out, _ = self.gru(x, h0)\n",
        "\n",
        "        # Decode the hidden state of the last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "# Initialize the GRU model\n",
        "gru_model = BinaryGRUModel(input_dim, hidden_dim, num_layers)\n",
        "\n",
        "# Define loss and optimizer for GRU model\n",
        "gru_optimizer = optim.Adam(gru_model.parameters(), lr=0.001)  # Adam optimizer for GRU model\n",
        "\n",
        "# Reuse the `train_model` function\n",
        "def train_gru_model(model, train_loader, val_loader, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for inputs, labels in train_loader:\n",
        "\n",
        "            gru_optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.squeeze(), labels)\n",
        "            loss.backward()\n",
        "            gru_optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                val_loss += criterion(outputs.squeeze(), labels).item()\n",
        "                all_preds.append(outputs.cpu().numpy())\n",
        "                all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        all_preds = np.concatenate(all_preds)\n",
        "        all_labels = np.concatenate(all_labels)\n",
        "\n",
        "        # Convert probabilities to binary predictions (0 or 1) using a threshold of 0.5\n",
        "        all_preds_binary = (all_preds > 0.5).astype(int)\n",
        "\n",
        "        # Calculate and print the confusion matrix\n",
        "        cm = confusion_matrix(all_labels, all_preds_binary)\n",
        "        print(f'Confusion Matrix for Epoch {epoch+1}:')\n",
        "        print(cm)\n",
        "\n",
        "        # Compute ROC AUC score\n",
        "        auc = roc_auc_score(all_labels, all_preds)\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}, ROC AUC: {auc:.4f}')\n",
        "\n",
        "# Train the GRU model\n",
        "num_epochs = 10  # Adjust as needed\n",
        "train_gru_model(gru_model, train_loader, val_loader, num_epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iK6kUxc1nyJ1",
        "outputId": "f0971a2f-b61e-4ea9-f2d7-36af44f6d52f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix for Epoch 1:\n",
            "[[167  46]\n",
            " [ 51 154]]\n",
            "Epoch 1, Loss: 0.6512, Val Loss: 0.5217, ROC AUC: 0.8388\n",
            "Confusion Matrix for Epoch 2:\n",
            "[[204   9]\n",
            " [  2 203]]\n",
            "Epoch 2, Loss: 0.3238, Val Loss: 0.0809, ROC AUC: 0.9988\n",
            "Confusion Matrix for Epoch 3:\n",
            "[[210   3]\n",
            " [  0 205]]\n",
            "Epoch 3, Loss: 0.0581, Val Loss: 0.0313, ROC AUC: 0.9998\n",
            "Confusion Matrix for Epoch 4:\n",
            "[[212   1]\n",
            " [  0 205]]\n",
            "Epoch 4, Loss: 0.0264, Val Loss: 0.0107, ROC AUC: 1.0000\n",
            "Confusion Matrix for Epoch 5:\n",
            "[[213   0]\n",
            " [  0 205]]\n",
            "Epoch 5, Loss: 0.0138, Val Loss: 0.0028, ROC AUC: 1.0000\n",
            "Confusion Matrix for Epoch 6:\n",
            "[[213   0]\n",
            " [  0 205]]\n",
            "Epoch 6, Loss: 0.0095, Val Loss: 0.0015, ROC AUC: 1.0000\n",
            "Confusion Matrix for Epoch 7:\n",
            "[[213   0]\n",
            " [  0 205]]\n",
            "Epoch 7, Loss: 0.0042, Val Loss: 0.0010, ROC AUC: 1.0000\n",
            "Confusion Matrix for Epoch 8:\n",
            "[[213   0]\n",
            " [  0 205]]\n",
            "Epoch 8, Loss: 0.0015, Val Loss: 0.0007, ROC AUC: 1.0000\n",
            "Confusion Matrix for Epoch 9:\n",
            "[[213   0]\n",
            " [  0 205]]\n",
            "Epoch 9, Loss: 0.0013, Val Loss: 0.0006, ROC AUC: 1.0000\n",
            "Confusion Matrix for Epoch 10:\n",
            "[[213   0]\n",
            " [  0 205]]\n",
            "Epoch 10, Loss: 0.0006, Val Loss: 0.0004, ROC AUC: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a CNN model for binary classification\n",
        "import torch.nn.functional as F\n",
        "class BinaryCNNModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_channels, kernel_size):\n",
        "        super(BinaryCNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=num_channels, kernel_size=kernel_size)\n",
        "        self.conv2 = nn.Conv1d(in_channels=num_channels, out_channels=num_channels*2, kernel_size=kernel_size)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "        self.fc1 = nn.Linear(320, 64)  # Adjust based on actual flattened length\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # Change shape to [batch_size, num_features, num_earthquakes]\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)  # Flatten the output\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.sigmoid(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "# Initialize the CNN model\n",
        "input_dim = 6  # Number of features per earthquake\n",
        "num_channels = 32  # Number of output channels for the first convolution layer\n",
        "kernel_size = 3  # Kernel size for the convolution layers\n",
        "\n",
        "cnn_model = BinaryCNNModel(input_dim, num_channels, kernel_size)\n",
        "\n",
        "\n",
        "# Define loss and optimizer for CNN model\n",
        "cnn_optimizer = optim.Adagrad(cnn_model.parameters(), lr=0.0001)  # Adam optimizer for CNN model\n",
        "\n",
        "# Reuse the `train_model` function\n",
        "def train_cnn_model(model, train_loader, val_loader, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for inputs, labels in train_loader:\n",
        "\n",
        "            cnn_optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.squeeze(), labels)\n",
        "            loss.backward()\n",
        "            cnn_optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                val_loss += criterion(outputs.squeeze(), labels).item()\n",
        "                all_preds.append(outputs.cpu().numpy())\n",
        "                all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        all_preds = np.concatenate(all_preds)\n",
        "        all_labels = np.concatenate(all_labels)\n",
        "\n",
        "        # Convert probabilities to binary predictions (0 or 1) using a threshold of 0.5\n",
        "        all_preds_binary = (all_preds > 0.5).astype(int)\n",
        "\n",
        "        # Calculate and print the confusion matrix\n",
        "        cm = confusion_matrix(all_labels, all_preds_binary)\n",
        "\n",
        "\n",
        "        # Compute ROC AUC score\n",
        "        auc = roc_auc_score(all_labels, all_preds)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "                print(f'Epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}, Val Loss: {val_loss:.4f}, ROC AUC: {auc:.4f}')\n",
        "                print(f'Confusion Matrix for Epoch {epoch+1}:')\n",
        "                print(cm)\n",
        "# Train the CNN model\n",
        "num_epochs = 100  # Adjust as needed\n",
        "train_cnn_model(cnn_model, train_loader, val_loader, num_epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Jmp7wVkoOpb",
        "outputId": "6c500136-5b18-4134-cfb6-d47e546f95a5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Loss: 0.6625, Val Loss: 0.6653, ROC AUC: 0.8651\n",
            "Confusion Matrix for Epoch 10:\n",
            "[[  0 213]\n",
            " [  0 205]]\n",
            "Epoch 20, Loss: 0.6463, Val Loss: 0.6531, ROC AUC: 0.8834\n",
            "Confusion Matrix for Epoch 20:\n",
            "[[  0 213]\n",
            " [  0 205]]\n",
            "Epoch 30, Loss: 0.6368, Val Loss: 0.6417, ROC AUC: 0.8937\n",
            "Confusion Matrix for Epoch 30:\n",
            "[[  6 207]\n",
            " [  0 205]]\n",
            "Epoch 40, Loss: 0.6201, Val Loss: 0.6303, ROC AUC: 0.9007\n",
            "Confusion Matrix for Epoch 40:\n",
            "[[ 21 192]\n",
            " [  0 205]]\n",
            "Epoch 50, Loss: 0.6151, Val Loss: 0.6184, ROC AUC: 0.9058\n",
            "Confusion Matrix for Epoch 50:\n",
            "[[ 49 164]\n",
            " [  1 204]]\n",
            "Epoch 60, Loss: 0.6022, Val Loss: 0.6064, ROC AUC: 0.9092\n",
            "Confusion Matrix for Epoch 60:\n",
            "[[ 66 147]\n",
            " [  1 204]]\n",
            "Epoch 70, Loss: 0.5901, Val Loss: 0.5941, ROC AUC: 0.9125\n",
            "Confusion Matrix for Epoch 70:\n",
            "[[ 86 127]\n",
            " [  1 204]]\n",
            "Epoch 80, Loss: 0.5753, Val Loss: 0.5819, ROC AUC: 0.9154\n",
            "Confusion Matrix for Epoch 80:\n",
            "[[ 98 115]\n",
            " [  2 203]]\n",
            "Epoch 90, Loss: 0.5589, Val Loss: 0.5697, ROC AUC: 0.9184\n",
            "Confusion Matrix for Epoch 90:\n",
            "[[113 100]\n",
            " [  4 201]]\n",
            "Epoch 100, Loss: 0.5586, Val Loss: 0.5577, ROC AUC: 0.9213\n",
            "Confusion Matrix for Epoch 100:\n",
            "[[125  88]\n",
            " [  6 199]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define a CNN model with dilation for binary classification\n",
        "class BinaryCNNModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_channels, kernel_size, dilation_rate=1):\n",
        "        super(BinaryCNNModel, self).__init__()\n",
        "        # Adding dilation to convolutional layers\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=num_channels,\n",
        "                               kernel_size=kernel_size, dilation=dilation_rate)\n",
        "        self.conv2 = nn.Conv1d(in_channels=num_channels, out_channels=num_channels * 2,\n",
        "                               kernel_size=kernel_size, dilation=dilation_rate)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "        self.fc1 = nn.Linear(64 * 4, 64)  # Adjusted based on the flattened size\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # Change shape to [batch_size, num_features, num_earthquakes]\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # Flatten the output\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.sigmoid(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Initialize the CNN model with dilation\n",
        "input_dim = 6  # Number of features per earthquake\n",
        "num_channels = 32  # Number of output channels for the first convolution layer\n",
        "kernel_size = 3  # Kernel size for the convolution layers\n",
        "dilation_rate = 2  # Dilation rate for the convolution layers\n",
        "\n",
        "cnn_model = BinaryCNNModel(input_dim, num_channels, kernel_size, dilation_rate)\n",
        "# cnn_model = cnn_model.to('cuda')  # Move model to GPU\n",
        "\n",
        "# Define loss and optimizer for CNN model\n",
        "criterion = nn.BCELoss()\n",
        "cnn_optimizer = optim.Adam(cnn_model.parameters(), lr=0.0001)  # Adam optimizer for CNN model\n",
        "\n",
        "# Reuse the `train_model` function\n",
        "def train_cnn_model(model, train_loader, val_loader, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            # inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "            cnn_optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.squeeze(), labels)\n",
        "            loss.backward()\n",
        "            cnn_optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                # inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "                outputs = model(inputs)\n",
        "                val_loss += criterion(outputs.squeeze(), labels).item()\n",
        "                all_preds.append(outputs.cpu().numpy())\n",
        "                all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        all_preds = np.concatenate(all_preds)\n",
        "        all_labels = np.concatenate(all_labels)\n",
        "\n",
        "        # Convert probabilities to binary predictions (0 or 1) using a threshold of 0.5\n",
        "        all_preds_binary = (all_preds > 0.5).astype(int)\n",
        "\n",
        "        # Calculate and print the confusion matrix\n",
        "        cm = confusion_matrix(all_labels, all_preds_binary)\n",
        "\n",
        "        # Compute ROC AUC score\n",
        "        auc = roc_auc_score(all_labels, all_preds)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}, Val Loss: {val_loss:.4f}, ROC AUC: {auc:.4f}')\n",
        "            print(f'Confusion Matrix for Epoch {epoch+1}:')\n",
        "            print(cm)\n",
        "\n",
        "# Train the CNN model\n",
        "num_epochs = 100  # Adjust as needed\n",
        "train_cnn_model(cnn_model, train_loader, val_loader, num_epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2pStFeqpKIr",
        "outputId": "b53cf8a2-c09b-4ec7-d818-76a89375b24b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Loss: 0.3279, Val Loss: 0.2967, ROC AUC: 0.9675\n",
            "Confusion Matrix for Epoch 10:\n",
            "[[193  20]\n",
            " [ 16 189]]\n",
            "Epoch 20, Loss: 0.0916, Val Loss: 0.0918, ROC AUC: 0.9952\n",
            "Confusion Matrix for Epoch 20:\n",
            "[[207   6]\n",
            " [  2 203]]\n",
            "Epoch 30, Loss: 0.0502, Val Loss: 0.0549, ROC AUC: 0.9987\n",
            "Confusion Matrix for Epoch 30:\n",
            "[[209   4]\n",
            " [  2 203]]\n",
            "Epoch 40, Loss: 0.0297, Val Loss: 0.0325, ROC AUC: 0.9996\n",
            "Confusion Matrix for Epoch 40:\n",
            "[[210   3]\n",
            " [  2 203]]\n",
            "Epoch 50, Loss: 0.0182, Val Loss: 0.0239, ROC AUC: 0.9998\n",
            "Confusion Matrix for Epoch 50:\n",
            "[[211   2]\n",
            " [  2 203]]\n",
            "Epoch 60, Loss: 0.0135, Val Loss: 0.0189, ROC AUC: 0.9999\n",
            "Confusion Matrix for Epoch 60:\n",
            "[[211   2]\n",
            " [  0 205]]\n",
            "Epoch 70, Loss: 0.0072, Val Loss: 0.0144, ROC AUC: 0.9999\n",
            "Confusion Matrix for Epoch 70:\n",
            "[[211   2]\n",
            " [  0 205]]\n",
            "Epoch 80, Loss: 0.0068, Val Loss: 0.0118, ROC AUC: 1.0000\n",
            "Confusion Matrix for Epoch 80:\n",
            "[[212   1]\n",
            " [  0 205]]\n",
            "Epoch 90, Loss: 0.0032, Val Loss: 0.0096, ROC AUC: 1.0000\n",
            "Confusion Matrix for Epoch 90:\n",
            "[[212   1]\n",
            " [  0 205]]\n",
            "Epoch 100, Loss: 0.0022, Val Loss: 0.0090, ROC AUC: 1.0000\n",
            "Confusion Matrix for Epoch 100:\n",
            "[[212   1]\n",
            " [  0 205]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "X = x_combined_normalized.reshape(x_combined_normalized.size(0), -1)  # Flatten sequences\n",
        "Y = y_combined\n",
        "# Split the resampled data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Random Forest model\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,            # Number of trees in the forest\n",
        "    max_depth=100000,                # Maximum depth of each tree\n",
        "    min_samples_split=5,         # Minimum number of samples required to split an internal node\n",
        "    min_samples_leaf=3,          # Minimum number of samples required to be at a leaf node\n",
        "    max_features='sqrt',         # Number of features to consider when looking for the best split\n",
        "    random_state=42              # For reproducibility\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_proba = rf_model.predict_proba(X_val)[:, 1]  # Get probabilities for the positive class\n",
        "y_pred = rf_model.predict(X_val)\n",
        "\n",
        "# Compute ROC AUC and Accuracy\n",
        "roc_auc = roc_auc_score(y_val, y_pred_proba)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "\n",
        "print(f'Random Forest ROC AUC: {roc_auc:.4f}')\n",
        "print(f'Random Forest Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Compute Confusion Matrix\n",
        "cm = confusion_matrix(y_val, y_pred)\n",
        "print(cm)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IJJASrIpeGd",
        "outputId": "d395f3c9-6bdb-4cbc-e0ff-0762d2336da2"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest ROC AUC: 0.9605\n",
            "Random Forest Accuracy: 0.9043\n",
            "[[192  21]\n",
            " [ 19 186]]\n"
          ]
        }
      ]
    }
  ]
}